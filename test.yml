

sealos run  labring/kubernetes:v1.25.0 labring/helm:v3.8.2 labring/calico:v3.24.1 \
  --masters 192.168.112.201,192.168.112.202 \
  --nodes 192.168.112.210 -p 000000



2022-11-06T15:12:34 info Start to create a new cluster: master [192.168.112.201 192.168.112.202], worker [192.168.112.210]
2022-11-06T15:12:34 info Executing pipeline Check in CreateProcessor.
2022-11-06T15:12:34 info checker:hostname [192.168.112.201:22 192.168.112.202:22 192.168.112.210:22]
2022-11-06T15:12:34 info checker:timeSync [192.168.112.201:22 192.168.112.202:22 192.168.112.210:22]
2022-11-06T15:12:35 info Executing pipeline PreProcess in CreateProcessor.
2022-11-06T15:12:35 info pulling images [labring/kubernetes:v1.25.0 labring/helm:v3.8.2 labring/calico:v3.24.1] for platform linux/amd64
Resolving "labring/kubernetes" using unqualified-search registries (/etc/containers/registries.conf)
Trying to pull docker.io/labring/kubernetes:v1.25.0...
Getting image source signatures
Copying blob 66c247fa15ae done
Copying config f5ce73040f done
Writing manifest to image destination
Storing signatures
f5ce73040f1569d7629e3d470ee3db00af79c48ac7baf6ff0ccf6bafbd7d013d
Resolving "labring/helm" using unqualified-search registries (/etc/containers/registries.conf)
Trying to pull docker.io/labring/helm:v3.8.2...
Getting image source signatures
Copying blob 53a6eade9e7e done
Copying config 1123e8b4b4 done
Writing manifest to image destination
Storing signatures
1123e8b4b455ed291f3ec7273af62e49458fe3dd141f5e7cb2a4243d6284deec
Resolving "labring/calico" using unqualified-search registries (/etc/containers/registries.conf)
Trying to pull docker.io/labring/calico:v3.24.1...
Getting image source signatures
Copying blob f9de59270f64 done
Copying config e2122fc58f done
Writing manifest to image destination
Storing signatures
e2122fc58fd32f1c93ac75da5c473aed746f1ad9b31a73d1f81a0579b96e775b
default-i5hbzeg6
default-w7sn5z4i
default-lnqlj8s9
2022-11-06T15:14:54 info Executing pipeline RunConfig in CreateProcessor.
2022-11-06T15:14:54 info Executing pipeline MountRootfs in CreateProcessor.
[1/1]copying files to 192.168.112.210:22  25% [==>            ] (1/4, 2 it/s) [0s:1s]which: no docker in (/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin)
 WARN [2022-11-06 15:15:01] >> Replace disable_apparmor = false to disable_apparmor = true
 INFO [2022-11-06 15:15:01] >> check root,port,cri success
Created symlink from /etc/systemd/system/multi-user.target.wants/containerd.service to /etc/systemd/system/containerd.service.
[1/1]copying files to 192.168.112.202:22  50% [======>        ] (2/4, 33 it/min) [3s:3s] INFO [2022-11-06 15:15:04] >> Health check containerd!
 INFO [2022-11-06 15:15:04] >> containerd is running
 INFO [2022-11-06 15:15:04] >> init containerd success
Created symlink from /etc/systemd/system/multi-user.target.wants/image-cri-shim.service to /etc/systemd/system/image-cri-shim.service.
 INFO [2022-11-06 15:15:04] >> Health check image-cri-shim!
 INFO [2022-11-06 15:15:04] >> image-cri-shim is running
 INFO [2022-11-06 15:15:04] >> init shim success
127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
::1         localhost localhost.localdomain localhost6 localhost6.localdomain6
* Applying /usr/lib/sysctl.d/00-system.conf ...
net.bridge.bridge-nf-call-ip6tables = 0
net.bridge.bridge-nf-call-iptables = 0
net.bridge.bridge-nf-call-arptables = 0
* Applying /usr/lib/sysctl.d/10-default-yama-scope.conf ...
kernel.yama.ptrace_scope = 0
* Applying /usr/lib/sysctl.d/50-default.conf ...
kernel.sysrq = 16
kernel.core_uses_pid = 1
kernel.kptr_restrict = 1
net.ipv4.conf.default.rp_filter = 1
net.ipv4.conf.all.rp_filter = 1
net.ipv4.conf.default.accept_source_route = 0
net.ipv4.conf.all.accept_source_route = 0
net.ipv4.conf.default.promote_secondaries = 1
net.ipv4.conf.all.promote_secondaries = 1
fs.protected_hardlinks = 1
fs.protected_symlinks = 1
* Applying /etc/sysctl.d/99-sysctl.conf ...
* Applying /etc/sysctl.d/k8s.conf ...
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.conf.all.rp_filter = 0
* Applying /etc/sysctl.conf ...
net.ipv4.ip_forward = 1
SELINUX=enforcing
Created symlink from /etc/systemd/system/multi-user.target.wants/kubelet.service to /etc/systemd/system/kubelet.service.
 INFO [2022-11-06 15:15:05] >> init kube success
 INFO [2022-11-06 15:15:05] >> init containerd rootfs success
192.168.112.202:22: which: no docker in (/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin)
192.168.112.202:22:  WARN [2022-11-06 15:15:39] >> Replace disable_apparmor = false to disable_apparmor = true
192.168.112.202:22:  INFO [2022-11-06 15:15:39] >> check root,port,cri success
192.168.112.210:22: which: no docker in (/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin)
192.168.112.210:22:  WARN [2022-11-06 15:15:39] >> Replace disable_apparmor = false to disable_apparmor = true
192.168.112.210:22:  INFO [2022-11-06 15:15:39] >> check root,port,cri success
192.168.112.202:22: Created symlink from /etc/systemd/system/multi-user.target.wants/containerd.service to /etc/systemd/system/containerd.service.
192.168.112.210:22: Created symlink from /etc/systemd/system/multi-user.target.wants/containerd.service to /etc/systemd/system/containerd.service.
192.168.112.202:22:  INFO [2022-11-06 15:15:42] >> Health check containerd!
192.168.112.202:22:  INFO [2022-11-06 15:15:42] >> containerd is running
192.168.112.202:22:  INFO [2022-11-06 15:15:42] >> init containerd success
192.168.112.202:22: Created symlink from /etc/systemd/system/multi-user.target.wants/image-cri-shim.service to /etc/systemd/system/image-cri-shim.service.
192.168.112.202:22:  INFO [2022-11-06 15:15:42] >> Health check image-cri-shim!
192.168.112.202:22:  INFO [2022-11-06 15:15:42] >> image-cri-shim is running
192.168.112.202:22:  INFO [2022-11-06 15:15:42] >> init shim success
192.168.112.202:22: 127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
192.168.112.202:22: ::1         localhost localhost.localdomain localhost6 localhost6.localdomain6
192.168.112.202:22: * Applying /usr/lib/sysctl.d/00-system.conf ...
192.168.112.202:22: net.bridge.bridge-nf-call-ip6tables = 0
192.168.112.202:22: net.bridge.bridge-nf-call-iptables = 0
192.168.112.202:22: net.bridge.bridge-nf-call-arptables = 0
192.168.112.202:22: * Applying /usr/lib/sysctl.d/10-default-yama-scope.conf ...
192.168.112.202:22: kernel.yama.ptrace_scope = 0
192.168.112.202:22: * Applying /usr/lib/sysctl.d/50-default.conf ...
192.168.112.202:22: kernel.sysrq = 16
192.168.112.202:22: kernel.core_uses_pid = 1
192.168.112.202:22: kernel.kptr_restrict = 1
192.168.112.202:22: net.ipv4.conf.default.rp_filter = 1
192.168.112.202:22: net.ipv4.conf.all.rp_filter = 1
192.168.112.202:22: net.ipv4.conf.default.accept_source_route = 0
192.168.112.202:22: net.ipv4.conf.all.accept_source_route = 0
192.168.112.202:22: net.ipv4.conf.default.promote_secondaries = 1
192.168.112.202:22: net.ipv4.conf.all.promote_secondaries = 1
192.168.112.202:22: fs.protected_hardlinks = 1
192.168.112.202:22: fs.protected_symlinks = 1
192.168.112.202:22: * Applying /etc/sysctl.d/99-sysctl.conf ...
192.168.112.202:22: * Applying /etc/sysctl.d/k8s.conf ...
192.168.112.202:22: net.bridge.bridge-nf-call-ip6tables = 1
192.168.112.202:22: net.bridge.bridge-nf-call-iptables = 1
192.168.112.202:22: net.ipv4.conf.all.rp_filter = 0
192.168.112.202:22: * Applying /etc/sysctl.conf ...
192.168.112.202:22: net.ipv4.ip_forward = 1
192.168.112.202:22: SELINUX=enforcing
192.168.112.210:22:  INFO [2022-11-06 15:15:43] >> Health check containerd!
192.168.112.210:22:  INFO [2022-11-06 15:15:43] >> containerd is running
192.168.112.210:22:  INFO [2022-11-06 15:15:43] >> init containerd success
192.168.112.210:22: Created symlink from /etc/systemd/system/multi-user.target.wants/image-cri-shim.service to /etc/systemd/system/image-cri-shim.service.
192.168.112.210:22:  INFO [2022-11-06 15:15:43] >> Health check image-cri-shim!
192.168.112.210:22:  INFO [2022-11-06 15:15:43] >> image-cri-shim is running
192.168.112.210:22:  INFO [2022-11-06 15:15:43] >> init shim success
192.168.112.210:22: 127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
192.168.112.210:22: ::1         localhost localhost.localdomain localhost6 localhost6.localdomain6
192.168.112.210:22: * Applying /usr/lib/sysctl.d/00-system.conf ...
192.168.112.210:22: net.bridge.bridge-nf-call-ip6tables = 0
192.168.112.210:22: net.bridge.bridge-nf-call-iptables = 0
192.168.112.210:22: net.bridge.bridge-nf-call-arptables = 0
192.168.112.210:22: * Applying /usr/lib/sysctl.d/10-default-yama-scope.conf ...
192.168.112.210:22: kernel.yama.ptrace_scope = 0
192.168.112.210:22: * Applying /usr/lib/sysctl.d/50-default.conf ...
192.168.112.210:22: kernel.sysrq = 16
192.168.112.210:22: kernel.core_uses_pid = 1
192.168.112.210:22: kernel.kptr_restrict = 1
192.168.112.210:22: net.ipv4.conf.default.rp_filter = 1
192.168.112.210:22: net.ipv4.conf.all.rp_filter = 1
192.168.112.210:22: net.ipv4.conf.default.accept_source_route = 0
192.168.112.210:22: net.ipv4.conf.all.accept_source_route = 0
192.168.112.210:22: net.ipv4.conf.default.promote_secondaries = 1
192.168.112.210:22: net.ipv4.conf.all.promote_secondaries = 1
192.168.112.210:22: fs.protected_hardlinks = 1
192.168.112.210:22: fs.protected_symlinks = 1
192.168.112.210:22: * Applying /etc/sysctl.d/99-sysctl.conf ...
192.168.112.210:22: * Applying /etc/sysctl.d/k8s.conf ...
192.168.112.210:22: net.bridge.bridge-nf-call-ip6tables = 1
192.168.112.210:22: net.bridge.bridge-nf-call-iptables = 1
192.168.112.210:22: net.ipv4.conf.all.rp_filter = 0
192.168.112.210:22: * Applying /etc/sysctl.conf ...
192.168.112.210:22: net.ipv4.ip_forward = 1
192.168.112.210:22: SELINUX=enforcing
192.168.112.202:22: Created symlink from /etc/systemd/system/multi-user.target.wants/kubelet.service to /etc/systemd/system/kubelet.service.
192.168.112.202:22:  INFO [2022-11-06 15:15:44] >> init kube success
192.168.112.202:22:  INFO [2022-11-06 15:15:44] >> init containerd rootfs success
192.168.112.210:22: Created symlink from /etc/systemd/system/multi-user.target.wants/kubelet.service to /etc/systemd/system/kubelet.service.
192.168.112.210:22:  INFO [2022-11-06 15:15:45] >> init kube success
192.168.112.210:22:  INFO [2022-11-06 15:15:45] >> init containerd rootfs success
2022-11-06T15:15:45 info Executing pipeline Init in CreateProcessor.
2022-11-06T15:15:45 info start to copy kubeadm config to master0
2022-11-06T15:15:45 info start to generate cert and kubeConfig...
2022-11-06T15:15:45 info start to generator cert and copy to masters...
2022-11-06T15:15:45 info apiserver altNames : {map[apiserver.cluster.local:apiserver.cluster.local k8s-master1:k8s-master1 kubernetes:kubernetes kubernetes.default:kubernetes.default kubernetes.default.svc:kubernetes.default.svc kubernetes.default.svc.cluster.local:kubernetes.default.svc.cluster.local localhost:localhost] map[10.103.97.2:10.103.97.2 10.96.0.1:10.96.0.1 127.0.0.1:127.0.0.1 192.168.112.201:192.168.112.201 192.168.112.202:192.168.112.202]}
2022-11-06T15:15:45 info Etcd altnames : {map[k8s-master1:k8s-master1 localhost:localhost] map[127.0.0.1:127.0.0.1 192.168.112.201:192.168.112.201 ::1:::1]}, commonName : k8s-master1
2022-11-06T15:15:47 info start to copy etc pki files to masters
2022-11-06T15:15:47 info start to create kubeconfig...
2022-11-06T15:15:48 info start to copy kubeconfig files to masters
2022-11-06T15:15:48 info start to copy static files to masters
2022-11-06T15:15:48 info start to apply registry
Created symlink from /etc/systemd/system/multi-user.target.wants/registry.service to /etc/systemd/system/registry.service.
 INFO [2022-11-06 15:15:48] >> Health check registry!
 INFO [2022-11-06 15:15:48] >> registry is running
 INFO [2022-11-06 15:15:48] >> init registry success
2022-11-06T15:15:48 info start to init master0...
2022-11-06T15:15:48 info registry auth in node 192.168.112.201:22
2022-11-06T15:15:49 info domain sealos.hub:192.168.112.201 append success
2022-11-06T15:15:49 info domain apiserver.cluster.local:192.168.112.201 append success
W1106 15:15:49.436135   35938 initconfiguration.go:119] Usage of CRI endpoints without URL scheme is deprecated and can cause kubelet errors in the future. Automatically prepending scheme "unix" to the "criSocket" with value "/run/containerd/containerd.sock". Please update your configuration!
[init] Using Kubernetes version: v1.25.0
[preflight] Running pre-flight checks
	[WARNING FileExisting-socat]: socat not found in system path
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
[certs] Using certificateDir folder "/etc/kubernetes/pki"
[certs] Using existing ca certificate authority
[certs] Using existing apiserver certificate and key on disk
[certs] Using existing apiserver-kubelet-client certificate and key on disk
[certs] Using existing front-proxy-ca certificate authority
[certs] Using existing front-proxy-client certificate and key on disk
[certs] Using existing etcd/ca certificate authority
[certs] Using existing etcd/server certificate and key on disk
[certs] Using existing etcd/peer certificate and key on disk
[certs] Using existing etcd/healthcheck-client certificate and key on disk
[certs] Using existing apiserver-etcd-client certificate and key on disk
[certs] Using the existing "sa" key
[kubeconfig] Using kubeconfig folder "/etc/kubernetes"
[kubeconfig] Using existing kubeconfig file: "/etc/kubernetes/admin.conf"
[kubeconfig] Using existing kubeconfig file: "/etc/kubernetes/kubelet.conf"
W1106 15:16:01.154683   35938 kubeconfig.go:249] a kubeconfig file "/etc/kubernetes/controller-manager.conf" exists already but has an unexpected API Server URL: expected: https://192.168.112.201:6443, got: https://apiserver.cluster.local:6443
[kubeconfig] Using existing kubeconfig file: "/etc/kubernetes/controller-manager.conf"
W1106 15:16:01.240404   35938 kubeconfig.go:249] a kubeconfig file "/etc/kubernetes/scheduler.conf" exists already but has an unexpected API Server URL: expected: https://192.168.112.201:6443, got: https://apiserver.cluster.local:6443
[kubeconfig] Using existing kubeconfig file: "/etc/kubernetes/scheduler.conf"
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Starting the kubelet
[control-plane] Using manifest folder "/etc/kubernetes/manifests"
[control-plane] Creating static Pod manifest for "kube-apiserver"
[control-plane] Creating static Pod manifest for "kube-controller-manager"
[control-plane] Creating static Pod manifest for "kube-scheduler"
[etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s
[apiclient] All control plane components are healthy after 7.006560 seconds
[upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
[kubelet] Creating a ConfigMap "kubelet-config" in namespace kube-system with the configuration for the kubelets in the cluster
[upload-certs] Skipping phase. Please see --upload-certs
[mark-control-plane] Marking the node k8s-master1 as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
[mark-control-plane] Marking the node k8s-master1 as control-plane by adding the taints [node-role.kubernetes.io/control-plane:NoSchedule]
[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
[bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes
[bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
[bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
[bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
[bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
[kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
[addons] Applied essential addon: CoreDNS
[addons] Applied essential addon: kube-proxy

Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

You can now join any number of control-plane nodes by copying certificate authorities
and service account keys on each node and then running the following as root:

  kubeadm join apiserver.cluster.local:6443 --token <value withheld> \
	--discovery-token-ca-cert-hash sha256:3a44b39a7a74cf2bb399d4cc783bd686036b1a490c08de73e912906c592efba5 \
	--control-plane --certificate-key <value withheld>

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join apiserver.cluster.local:6443 --token <value withheld> \
	--discovery-token-ca-cert-hash sha256:3a44b39a7a74cf2bb399d4cc783bd686036b1a490c08de73e912906c592efba5
2022-11-06T15:16:09 info Executing pipeline Join in CreateProcessor.
2022-11-06T15:16:09 info [192.168.112.202:22] will be added as master
2022-11-06T15:16:09 info start to init filesystem join masters...
2022-11-06T15:16:10 info start to copy static files to masters
2022-11-06T15:16:10 info start to copy kubeconfig files to masters
2022-11-06T15:16:11 info start to copy etc pki files to masters (1/1, 3 it/s)
2022-11-06T15:16:19 info start to get kubernetes token...
2022-11-06T15:16:21 info start to copy kubeadm join config to master: 192.168.112.202:22
2022-11-06T15:16:22 info start to join 192.168.112.202:22 as master1, 3 it/s)
2022-11-06T15:16:22 info registry auth in node 192.168.112.202:22
192.168.112.202:22: 2022-11-06T15:16:23 info domain sealos.hub:192.168.112.201 append success
2022-11-06T15:16:23 info start to generator cert 192.168.112.202:22 as master
192.168.112.202:22: 2022-11-06T15:16:23 info apiserver altNames : {map[apiserver.cluster.local:apiserver.cluster.local k8s-master2:k8s-master2 kubernetes:kubernetes kubernetes.default:kubernetes.default kubernetes.default.svc:kubernetes.default.svc kubernetes.default.svc.cluster.local:kubernetes.default.svc.cluster.local localhost:localhost] map[10.103.97.2:10.103.97.2 10.96.0.1:10.96.0.1 127.0.0.1:127.0.0.1 192.168.112.201:192.168.112.201 192.168.112.202:192.168.112.202]}
192.168.112.202:22: 2022-11-06T15:16:23 info Etcd altnames : {map[k8s-master2:k8s-master2 localhost:localhost] map[127.0.0.1:127.0.0.1 192.168.112.202:192.168.112.202 ::1:::1]}, commonName : k8s-master2
192.168.112.202:22: 2022-11-06T15:16:23 info sa.key sa.pub already exist
192.168.112.202:22: 2022-11-06T15:16:25 info domain apiserver.cluster.local:192.168.112.201 append success
192.168.112.202:22: W1106 15:16:25.258364   38634 initconfiguration.go:119] Usage of CRI endpoints without URL scheme is deprecated and can cause kubelet errors in the future. Automatically prepending scheme "unix" to the "criSocket" with value "/run/containerd/containerd.sock". Please update your configuration!
192.168.112.202:22: [preflight] Running pre-flight checks
192.168.112.202:22: 	[WARNING FileExisting-socat]: socat not found in system path
192.168.112.202:22: [preflight] Reading configuration from the cluster...
192.168.112.202:22: [preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'
192.168.112.202:22: [preflight] Running pre-flight checks before initializing the new control plane instance
192.168.112.202:22: [preflight] Pulling images required for setting up a Kubernetes cluster
192.168.112.202:22: [preflight] This might take a minute or two, depending on the speed of your internet connection
192.168.112.202:22: [preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
192.168.112.202:22: [download-certs] Downloading the certificates in Secret "kubeadm-certs" in the "kube-system" Namespace
192.168.112.202:22: [certs] Using certificateDir folder "/etc/kubernetes/pki"
192.168.112.202:22: [certs] Using the existing "etcd/peer" certificate and key
192.168.112.202:22: [certs] Using the existing "etcd/healthcheck-client" certificate and key
192.168.112.202:22: [certs] Using the existing "etcd/server" certificate and key
192.168.112.202:22: [certs] Using the existing "apiserver-etcd-client" certificate and key
192.168.112.202:22: [certs] Using the existing "apiserver-kubelet-client" certificate and key
192.168.112.202:22: [certs] Using the existing "apiserver" certificate and key
192.168.112.202:22: [certs] Using the existing "front-proxy-client" certificate and key
192.168.112.202:22: [certs] Valid certificates and keys now exist in "/etc/kubernetes/pki"
192.168.112.202:22: [certs] Using the existing "sa" key
192.168.112.202:22: [kubeconfig] Generating kubeconfig files
192.168.112.202:22: [kubeconfig] Using kubeconfig folder "/etc/kubernetes"
192.168.112.202:22: [kubeconfig] Using existing kubeconfig file: "/etc/kubernetes/admin.conf"
192.168.112.202:22: W1106 15:16:41.967275   38634 kubeconfig.go:249] a kubeconfig file "/etc/kubernetes/controller-manager.conf" exists already but has an unexpected API Server URL: expected: https://192.168.112.202:6443, got: https://apiserver.cluster.local:6443
192.168.112.202:22: [kubeconfig] Using existing kubeconfig file: "/etc/kubernetes/controller-manager.conf"
192.168.112.202:22: W1106 15:16:42.133355   38634 kubeconfig.go:249] a kubeconfig file "/etc/kubernetes/scheduler.conf" exists already but has an unexpected API Server URL: expected: https://192.168.112.202:6443, got: https://apiserver.cluster.local:6443
192.168.112.202:22: [kubeconfig] Using existing kubeconfig file: "/etc/kubernetes/scheduler.conf"
192.168.112.202:22: [control-plane] Using manifest folder "/etc/kubernetes/manifests"
192.168.112.202:22: [control-plane] Creating static Pod manifest for "kube-apiserver"
192.168.112.202:22: [control-plane] Creating static Pod manifest for "kube-controller-manager"
192.168.112.202:22: [control-plane] Creating static Pod manifest for "kube-scheduler"
192.168.112.202:22: [check-etcd] Checking that the etcd cluster is healthy
192.168.112.202:22: [kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
192.168.112.202:22: [kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
192.168.112.202:22: [kubelet-start] Starting the kubelet
192.168.112.202:22: [kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...
192.168.112.202:22: [etcd] Announced new etcd member joining to the existing etcd cluster
192.168.112.202:22: [etcd] Creating static Pod manifest for "etcd"
192.168.112.202:22: [etcd] Waiting for the new etcd member to join the cluster. This can take up to 40s
192.168.112.202:22: The 'update-status' phase is deprecated and will be removed in a future release. Currently it performs no operation
192.168.112.202:22: [mark-control-plane] Marking the node k8s-master2 as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
192.168.112.202:22: [mark-control-plane] Marking the node k8s-master2 as control-plane by adding the taints [node-role.kubernetes.io/control-plane:NoSchedule]
192.168.112.202:22:
192.168.112.202:22: This node has joined the cluster and a new control plane instance was created:
192.168.112.202:22:
192.168.112.202:22: * Certificate signing request was sent to apiserver and approval was received.
192.168.112.202:22: * The Kubelet was informed of the new secure connection details.
192.168.112.202:22: * Control plane label and taint were applied to the new node.
192.168.112.202:22: * The Kubernetes control plane instances scaled up.
192.168.112.202:22: * A new etcd member was added to the local/stacked etcd cluster.
192.168.112.202:22:
192.168.112.202:22: To start administering your cluster from this node, you need to run the following as a regular user:
192.168.112.202:22:
192.168.112.202:22: 	mkdir -p $HOME/.kube
192.168.112.202:22: 	sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
192.168.112.202:22: 	sudo chown $(id -u):$(id -g) $HOME/.kube/config
192.168.112.202:22:
192.168.112.202:22: Run 'kubectl get nodes' to see this node join the cluster.
192.168.112.202:22:
192.168.112.202:22: 2022-11-06T15:17:06 info domain apiserver.cluster.local delete success
192.168.112.202:22: 2022-11-06T15:17:06 info domain apiserver.cluster.local:192.168.112.202 append success
2022-11-06T15:17:06 info succeeded in joining 192.168.112.202:22 as master
2022-11-06T15:17:06 info [192.168.112.210:22] will be added as worker
2022-11-06T15:17:06 info start to get kubernetes token...
2022-11-06T15:17:06 info start to join 192.168.112.210:22 as worker
2022-11-06T15:17:06 info start to copy kubeadm join config to node: 192.168.112.210:22
192.168.112.210:22: 2022-11-06T15:17:08 info domain apiserver.cluster.local:10.103.97.2 append success
192.168.112.210:22: 2022-11-06T15:17:08 info domain lvscare.node.ip:192.168.112.210 append success
2022-11-06T15:17:08 info registry auth in node 192.168.112.210:22
192.168.112.210:22: 2022-11-06T15:17:09 info domain sealos.hub:192.168.112.201 append success
2022-11-06T15:17:09 info run ipvs once module: 192.168.112.210:22
192.168.112.210:22: 2022-11-06T15:17:09 info Trying to add route
192.168.112.210:22: 2022-11-06T15:17:09 info success to set route.(host:10.103.97.2, gateway:192.168.112.210)
2022-11-06T15:17:09 info start join node: 192.168.112.210:22
192.168.112.210:22: W1106 15:17:09.957992   39047 initconfiguration.go:119] Usage of CRI endpoints without URL scheme is deprecated and can cause kubelet errors in the future. Automatically prepending scheme "unix" to the "criSocket" with value "/run/containerd/containerd.sock". Please update your configuration!
192.168.112.210:22: [preflight] Running pre-flight checks
192.168.112.210:22: 	[WARNING FileExisting-socat]: socat not found in system path
192.168.112.210:22: [preflight] Reading configuration from the cluster...
192.168.112.210:22: [preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'
192.168.112.210:22: [kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
192.168.112.210:22: [kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
192.168.112.210:22: [kubelet-start] Starting the kubelet
192.168.112.210:22: [kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...
192.168.112.210:22:
192.168.112.210:22: This node has joined the cluster:
192.168.112.210:22: * Certificate signing request was sent to apiserver and a response was received.
192.168.112.210:22: * The Kubelet was informed of the new secure connection details.
192.168.112.210:22:
192.168.112.210:22: Run 'kubectl get nodes' on the control-plane to see this node join the cluster.
192.168.112.210:22:
2022-11-06T15:17:23 info succeeded in joining 192.168.112.210:22 as worker
2022-11-06T15:17:24 info start to sync lvscare static pod to node: 192.168.112.210:22 master: [192.168.112.201:6443 192.168.112.202:6443]
192.168.112.210:22: 2022-11-06T15:17:24 info generator lvscare static pod is success
2022-11-06T15:17:24 info Executing pipeline RunGuest in CreateProcessor.
2022-11-06T15:17:24 info guest cmd is cp opt/helm /usr/bin/
2022-11-06T15:17:24 info guest cmd is kubectl create namespace tigera-operator
namespace/tigera-operator created
2022-11-06T15:17:24 info guest cmd is helm install calico charts/calico --namespace tigera-operator
NAME: calico
LAST DEPLOYED: Sun Nov  6 15:17:26 2022
NAMESPACE: tigera-operator
STATUS: deployed
REVISION: 1
TEST SUITE: None
2022-11-06T15:17:27 info succeeded in creating a new cluster, enjoy it!
2022-11-06T15:17:27 info
      ___           ___           ___           ___       ___           ___
     /\  \         /\  \         /\  \         /\__\     /\  \         /\  \
    /::\  \       /::\  \       /::\  \       /:/  /    /::\  \       /::\  \
   /:/\ \  \     /:/\:\  \     /:/\:\  \     /:/  /    /:/\:\  \     /:/\ \  \
  _\:\~\ \  \   /::\~\:\  \   /::\~\:\  \   /:/  /    /:/  \:\  \   _\:\~\ \  \
 /\ \:\ \ \__\ /:/\:\ \:\__\ /:/\:\ \:\__\ /:/__/    /:/__/ \:\__\ /\ \:\ \ \__\
 \:\ \:\ \/__/ \:\~\:\ \/__/ \/__\:\/:/  / \:\  \    \:\  \ /:/  / \:\ \:\ \/__/
  \:\ \:\__\    \:\ \:\__\        \::/  /   \:\  \    \:\  /:/  /   \:\ \:\__\
   \:\/:/  /     \:\ \/__/        /:/  /     \:\  \    \:\/:/  /     \:\/:/  /
    \::/  /       \:\__\         /:/  /       \:\__\    \::/  /       \::/  /
     \/__/         \/__/         \/__/         \/__/     \/__/         \/__/

                  Website :https://www.sealos.io/
                  Address :github.com/labring/sealos
		BuildVersion: 4.1.4-rc1-21a6607e