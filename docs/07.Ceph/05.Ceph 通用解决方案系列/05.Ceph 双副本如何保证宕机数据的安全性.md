---
title: （5）Ceph 双副本如何保证宕机数据的安全性
date: 2023-10-07 15:16:30
permalink: /pages/e52b90/
categories:
  - Ceph
  - Ceph 通用解决方案系列
tags:
  - 
author: 
  name: brook-w
  link: https://github.com/brook-w
---

**场景一：**

生产环境一般都是三副本存储，但一些场景为了节省资源，将副本调整为 2 副本。两副本要求将数据分布在不同的机器上，防止集群故障时数据丢失。

我们为此添加 `Rack` 级的 `Bucket`，分别包含两个存储节点（以 `Host` 的 `Bucket`），然后以 `Rack` 为隔离域，保证两个副本分别落在不同的 `Rack` 上。

示意图如下： ![image](https://jsd.cdn.zzko.cn/gh/brook-w/image-hosting@master/ceph/image.473k8jwfhge0.webp)

## 1. 备份 crushmap

修改前数据备份，以防万一。crushmap 备份

```bash
ceph osd getcrushmap  -o backup.txt
```

恢复：

```
ceph osd setcrushmap  -i backup.txt
```

## 2. 操作

1. 创建机柜

```
ceph osd crush add-bucket rack1 rack
ceph osd crush add-bucket rack2 rack
```

2. 将 rack 移动到 root 下

```bash
ceph osd crush move rack1 root=default
ceph osd crush move rack2 root=default
```

3. 将主机移动到机柜中

```bash
ceph osd crush move node1 rack=rack1
ceph osd crush move node2 rack=rack2
```

4. 创建 rule

```bash
ceph osd crush rule create-simple testrule default rack firstn
```

5. 修改已经创建 pool 的 rule

luminus 以后版本设置 pool 规则的语法是

```bash
ceph osd pool set demo crush_rule testrule
```

luminus 以前版本设置 pool 规则的语法是

查看 rule 的 ID

```bash
ceph osd crush rule dump | grep testrul
1

ceph osd pool set demo crush_rule 1 
```

此处 1 是指在 rule 里 rule_ id 设置的值

6. 创建 pool

```bash
ceph osd pool create demo 64 64 replicated testrule
```

7. 在 ceph.conf 中加入防止集群重启 crushmap 被重置

```bash
osd crush update on start = false

# 默认是 true
[root@ceph1 ~]# ceph config get osd osd_crush_update_on_start
true
```

## 引用

- [https://blog.csdn.net/li4528503/article/details/106256638](https://blog.csdn.net/li4528503/article/details/106256638)