---
title: （4）指定 SSD 为主 OSD，HDD 为从 OSD
date: 2023-10-07 15:16:05
permalink: /pages/1b3a36/
categories:
  - Ceph
  - Ceph 通用解决方案系列
tags:
  - 
author: 
  name: brook-w
  link: https://github.com/brook-w
---

## 前言

### 场景
- 假设 1：当前集群环境中混用了 SSD 与 SATA 磁盘
- 假设 2：当前具有一批大小一样的 SATA 磁盘 其中 OSD.0 已经使用了很多年 其他 OSD 磁盘都是新采购，性能会比 OSD.0 更加好 实验室中有两批 sas 磁盘，一批是 7200 转磁盘，一批是 10000 转磁盘，

### 期望结果
- 集群中存在多种性能不一样的磁盘与主机，为了保证 `CEPH` 集群吞吐与性能可以得到更好的优化
- 保证了磁盘的数据容量一致的情况下优先使用性能更好的磁盘作出 IO 处理

![image](https://jsd.cdn.zzko.cn/gh/brook-w/image-hosting@master/ceph/image.5wd95l87ttk0.webp)

### 总结

`CEPH` 会认为每个磁盘能力一样，会平均每个磁盘的 `IO` 能力 (`primary affinity`) 与存储空间能力 (`weight`) `primary affinity` 参数可以在不改变磁盘存储能力的情况下，降低在 `CEPH` 集群使用过程中对该 `osd` 的认购 `primary pg` 能力

从上图看出来，`priamry osd` 在每次 `peering` 时，都会比 `secondary osd` 具有更多的 `IO`

## 操作

修改配置文件 `/etc/ceph/ceph.conf`

```bash
[mon]
mon_osd_allow_primary_affinity = true
```

重启 ceph-mon

```
ceph osd primary-affinity osd.4 0.5
```

验证： 修改权限之前：

```
ceph pg dump | grep active+clean  | grep '\[4' | wc -l
48
```

修改权重之后

```
ceph pg dump | grep active+clean  | grep '\[4' | wc -l
8
```

:::tip
对于复制池，此调整特别有用，因为默认情况下，读取操作由每个 `PG` 的主 `OSD` 提供服务（SSD + HDD）

[对于纠删码池，可以通过启用快速读取来提高读取操作的速度](https://docs.ceph.com/en/reef/rados/configuration/mon-config-ref/#pool-settings)


- osd_pool_default_ec_fast_read
    > Whether to turn on fast read on the pool or not. It will be used as the default setting of newly created erasure coded pools if fast_read is not specified at create time.
    >
    - type
        - bool
    - default
        - false
```bash
[root@ceph1 ~]# ceph config get osd osd_pool_default_ec_fast_read
false
```
:::

## 应用
- [https://docs.ceph.com/en/reef/rados/operations/crush-map/#primary-affinity](https://docs.ceph.com/en/reef/rados/operations/crush-map/#primary-affinity)