---
title: （2）Ceph 集群同时创建 SSD 和 HDD Pool 池
date: 2023-10-07 15:14:08
permalink: /pages/e7ab43/
categories:
  - Ceph
  - Ceph 通用解决方案系列
tags:
  - 
author: 
  name: brook-w
  link: https://github.com/brook-w
---


**存储快慢池**

场景： 存储节点上既有 `SATA` 盘也有 `SSD` 盘，把各节点的 `SSD` 和 `SATA` 分别整合成独立的存储池，为不同的应用供给不同性能的存储。

- 比如常见的云环境中的虚拟机实例，对于实时数据 `IO` 性能要求高，并且都是热数据，可以把这部分的存储需求放入 `SSD` 的存储池里
- 而对于备份、快照等冷数据应用，相对 `IO` 性能需求比较低，因此将其可以放在普通的由 `SATA` 盘组成的存储池里。

![image](https://jsd.cdn.zzko.cn/gh/brook-w/image-hosting@master/ceph/image.30wr6c5iwl80.webp)

**方法：** `Luminous` 版本的 `ceph` 新增了一个 `crush class` 的功能，可以为新增的设备指定为类，创建 `rule` 的时候直接指定 `class` 即可。 创建一个磁盘类型的 `class`，给磁盘标记 `class` 的统一标签，自动会根据 `class` 的类型创建一个树，根据树创建 `rule`，根据 `rule` 创建存储池，整个操作没有动 `crushmap` 的操作 增加或修改盘的时候，设置下属性即可

**该方案的优点：** 该方案的优点为充分利用 SSD 设备高性能的优势，将关键性业务或者 IO 敏感型业务全部放入高性能存储池，为客户提供性能优越的存储服务。

**该方案的缺点：** 该方案的缺点为受到成本限制，高性能存储池存储容量规模会比较小，只能针对性地对少数关键性业务或者 IO 敏感型业务进行服务质量保障，且业务一经规划部署至某个存储池后，不能在两个存储池之间进行自动转换，后期进行调整开销较大。

修改前集群拓扑：

```bash
[root@storage1 ~]# ceph osd tree
ID CLASS WEIGHT  TYPE NAME         STATUS REWEIGHT PRI-AFF
-1       0.52734 root default
-3       0.17578     host storage1
 0   hdd 0.07809         osd.0         up  1.00000 1.00000
 3   hdd 0.09769         osd.3         up  1.00000 1.00000
-5       0.17578     host storage2
 1   hdd 0.07809         osd.1         up  1.00000 1.00000
 4   hdd 0.09769         osd.4         up  1.00000 1.00000
-7       0.17578     host storage3
 2   hdd 0.07809         osd.2         up  1.00000 1.00000
 5   hdd 0.09769         osd.5         up  1.00000 1.00000
```

查看 class 类型

```bash
[root@storage1 ~]# ceph osd crush class ls
[
    "hdd"
]
```

删除默认的 class

```bash
[root@storage1 ~]# for i in {0..5} ; do ceph osd crush rm-device-class osd.$i;done
done removing class of osd(s): 0
done removing class of osd(s): 1
done removing class of osd(s): 2
done removing class of osd(s): 3
done removing class of osd(s): 4
done removing class of osd(s): 5

[root@storage1 ~]# ceph osd tree
ID CLASS WEIGHT  TYPE NAME         STATUS REWEIGHT PRI-AFF
-1       0.52734 root default
-3       0.17578     host storage1
 0       0.07809         osd.0         up  1.00000 1.00000
 3       0.09769         osd.3         up  1.00000 1.00000
-5       0.17578     host storage2
 1       0.07809         osd.1         up  1.00000 1.00000
 4       0.09769         osd.4         up  1.00000 1.00000
-7       0.17578     host storage3
 2       0.07809         osd.2         up  1.00000 1.00000
 5       0.09769         osd.5         up  1.00000 1.00000
```

将 osd 编号 0-2 标记为 SAS

```bash
for i in 0 1 2;do ceph osd crush set-device-class sas osd.$i;done
[root@storage1 ~]# ceph osd tree
ID CLASS WEIGHT  TYPE NAME         STATUS REWEIGHT PRI-AFF
-1       0.52734 root default
-3       0.17578     host storage1
 3       0.09769         osd.3         up  1.00000 1.00000
 0   sas 0.07809         osd.0         up  1.00000 1.00000
-5       0.17578     host storage2
 4       0.09769         osd.4         up  1.00000 1.00000
 1   sas 0.07809         osd.1         up  1.00000 1.00000
-7       0.17578     host storage3
 5       0.09769         osd.5         up  1.00000 1.00000
 2   sas 0.07809         osd.2         up  1.00000 1.00000

[root@storage1 ~]# ceph osd crush class ls
[
    "sas"
]
```

将 osd 编号 3-5 标记为 ssd

```bash
[root@storage1 ~]#  for i in {3..5} ;do ceph osd crush set-device-class ssd osd.$i;done
set osd(s) 3 to class 'ssd'
set osd(s) 4 to class 'ssd'
set osd(s) 5 to class 'ssd'

[root@storage1 ~]# ceph osd tree
ID CLASS WEIGHT  TYPE NAME         STATUS REWEIGHT PRI-AFF
-1       0.52734 root default
-3       0.17578     host storage1
 0   sas 0.07809         osd.0         up  1.00000 1.00000
 3   ssd 0.09769         osd.3         up  1.00000 1.00000
-5       0.17578     host storage2
 1   sas 0.07809         osd.1         up  1.00000 1.00000
 4   ssd 0.09769         osd.4         up  1.00000 1.00000
-7       0.17578     host storage3
 2   sas 0.07809         osd.2         up  1.00000 1.00000
 5   ssd 0.09769         osd.5         up  1.00000 1.00000

[root@storage1 ~]# ceph osd crush class ls
[
    "sas",
    "ssd"
]
```

创建个 ssd 规则：

```bash
[root@storage1 ~]# ceph osd crush rule create-replicated rule-ssd default host ssd
[root@storage1 ~]# ceph osd crush rule create-replicated rule-sas default host sas
```

格式如下：

```bash
ceph osd crush rule create-replicated <rule-name> <root> <failure-domain> <class>

[root@storage1 ~]# ceph osd crush rule ls
replicated_rule
rule-ssd
rule-sas
```

创建一个使用该 rule-ssd 规则的存储池：

```bash
[root@storage1 ~]# ceph osd pool create ssdpool 64 64 rule-ssd
pool 'ssdpool' created
```

创建一个使用该 rule-sas 规则的存储池：

```bash
[root@storage1 ~]# ceph osd pool create saspool 64 64 rule-sas
pool 'saspool' created
```

查看 ssdpool 的信息可以看到使用的 crush_rule 为 1，也就是 rule-ssd

```bash
[root@storage1 ~]# ceph osd pool ls detail | grep ssdpool
pool 6 'ssdpool' replicated size 3 min_size 2 crush_rule 1 object_hash rjenkins pg_num 64 pgp_num 64 last_change 219 flags hashpspool stripe_width 0

[root@storage1 ~]# ceph osd pool ls detail | grep saspool
pool 7 'saspool' replicated size 3 min_size 2 crush_rule 2 object_hash rjenkins pg_num 64 pgp_num 64 last_change 223 flags hashpspool stripe_width 0
```
