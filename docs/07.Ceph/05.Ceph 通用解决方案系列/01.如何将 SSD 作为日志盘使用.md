---
title: （1）如何将 SSD 作为日志盘使用
date: 2023-10-07 15:12:55
permalink: /pages/70401e/
categories:
  - Ceph
  - Ceph 通用解决方案系列
tags:
  - 
author: 
  name: brook-w
  link: https://github.com/brook-w
---

## 实现原理

假设磁盘正在执行一个写操作，此时由于发生磁盘错误，或者系统宕机、断电等其他原因，导致只有部分数据写入成功。这种情况就会出现磁盘上的数据有一部分是旧数据，另一部分是新写入的数据，使得磁盘数据不一致。 

`Ceph` 引入**事务与日志**，来实现数据写盘操作的原子性，并解决数据不一致的问题。即所谓的 **ceph 数据双写**：先把数据全部封装成一个事务，将其整体作为一条日志，写入 `ceph-osd journal`，然后再把数据定时回刷写入对象文件，将其持久化到 `ceph-osd filestore` 中。 基于以上过程，可以将 `SSD` 作为 `ceph-osd journal` 的底层存储设备，来加速写入性能。同时，由于 `SSD` 设备 `IO` 性能较高，可以将 `SSD` 划分成多个分区，以配比多个 `HDD` 设备使用。

![image](https://jsd.cdn.zzko.cn/gh/brook-w/image-hosting@master/ceph/image.1sj2t17f0xhc.webp)

**该方案的优点** 该方案的优点为使用高速的 `SSD` 设备加速 `ceph-osd journal` 的写入性能，尤其是对小块数据随机 `IO` 的场景，加速效果尤为明显。 

**该方案的缺点** 该方案的缺点为由于 `ceph-osd journal` 在实现逻辑上具有循环写入、定期回刷的特性，其对 SSD 设备容量空间的利用率很低。典型场景下，`SSD` 设备与 `HDD` 设备的配比为 `1:4`，而每块 `HDD` 设备一般只使用 `10GB` 的 `SSD` 设备分区，造成了 `SSD` 设备容量空间的浪费。

## 操作

### 1. [安装 Ceph 集群](/pages/5edf67/)

### 2. 实际操作

#### 2.1 创建 OSD（bluestore）

1. 创建逻辑卷
    ```bash
    vgcreate cache /dev/sdb
    lvcreate --size 100G --name db-lv-0 cache

    vgcreate cache /dev/sdb
    lvcreate --size 100G --name wal-lv-0 cache
    ```

2. 创建 OSD
    ```bash
    ceph-deploy osd create --bluestore storage1 --data /dev/sdc --block-db cache/db-lv-0 --block-wal cache/wal-lv-0
    ceph-deploy osd create --bluestore storage2 --data /dev/sdc --block-db cache/db-lv-0 --block-wal cache/wal-lv-0
    ceph-deploy osd create --bluestore storage3 --data /dev/sdc --block-db cache/db-lv-0 --block-wal cache/wal-lv-0
    ```

:::tip
[关于 filestore 和 bluestore 的区别这篇文章做了详细的说明，在有 ssd 的情况下 bluestore 优势比较明显](http://www.yuncunchu.org/portal.php?mod=view&amp;aid=74)
:::


:::tip wal & db 的大小问题

使用混合机械和固态硬盘设置时，block.db 为 Bluestore 创建足够大的逻辑卷非常重要 。通常，block.db 应该具有尽可能大的逻辑卷。 建议 block.db 尺寸不小于 4％ block。例如，如果 block 大小为 1TB，则 block.db 不应小于 40GB。 如果不使用快速和慢速设备的混合，则不需要为 block.db（或 block.wal）创建单独的逻辑卷。Bluestore 将在空间内自动管理这些内容 block。
:::

3. 验证

```
ceph health
HEALTH_OK
```