---
title: Ceph 集群优化
date: 2023-08-16 11:32:50
permalink: /pages/bc143f/
categories:
  - Ceph
tags:
  - 
author: 
  name: brook-w
  link: https://github.com/brook-w
---

## 1. 内核优化

### 1. [块设备优化](https://www.xmmup.com/linuxyouhuazhicipanyudureadahead.html)

```sh
blockdev -V
blockdev --report [设备]
blockdev [-v|-q] 命令 设备

--getsz                   获得 512-字节扇区的大小 
--setro                   设置只读 
--setrw                   设置读写 
--getro                   获得只读 
--getdiscardzeroes        获取 忽略零数据 支持状态 
--getss                   获得逻辑块(扇区)大小 
--getpbsz                 获得物理块(扇区)大小 
--getiomin                获得最小 I/O 大小 
--getioopt                获得最优 I/O 大小 
--getalignoff             获得对齐偏移字节数 
--getmaxsect              获得每次请求的最大扇区数 
--getbsz                  获得块大小 
--setbsz                  set blocksize on file descriptor opening the block device --getsize                 获得 32-bit 扇区数量(已废弃，请使用 --getsz) 
--getsize64               获得字节大小 
--setra                   设置 readahead 
--getra                   获取 readahead 
--setfra                  设置文件系统 readahead 
--getfra                  获取文件系统 readahead 
--flushbufs               刷新缓存 
--rereadpt                重新读取分区表
```

### 2. 块设备预读大小

```sh
/sbin/blockdev --getra /dev/sda
cat /sys/class/block/sda/queue/read_ahead_kb

blockdev --report
```




## 2. 网络配置

Ceph 集群中有两个网络（管理内网和外部网络）每个节点都有 4 个 10G 光口。

管理内网：
由于管理网络主要用于集群节点之间的通信、数据同步和控制命令传输，稳定性和可靠性非常重要。在这个网络上，推荐使用 "activebackup" 运行者模式。

- 冗余性：该模式提供了故障切换和冗余性，以确保在一个主接口出现故障时能够切换到备用接口，保持管理网络的连通性。
- 稳定性：管理网络的稳定性是至关重要的，因为它直接影响到集群节点之间的通信和控制。

外部网络：

外部网络通常用于与其他集群或外部系统进行通信，以及为 OpenStack 集群等提供网络连接。在这个网络上，你可以考虑使用 "loadbalance" 运行者模式。

- 带宽利用：外部网络通常需要支持大量的数据传输，负载均衡模式可以更好地利用多个 10G 光口的带宽，从而提高整体的网络性能。
- 高吞吐量："loadbalance" 模式适用于需要处理大量数据的场景，这对于外部网络非常适用。

activebackup

```
vi /etc/sysconfig/network-scripts/ifcfg-team0
DEVICE=team0
DEVICETYPE=Team
ONBOOT=yes
BOOTPROTO=none
IPADDR=<管理内网 IP>
NETMASK=<子网掩码>
GATEWAY=<网关地址>
DNS1=<首选 DNS 服务器>
DNS2=<备选 DNS 服务器>
TEAM_CONFIG='{"runner": {"name": "activebackup"}}'
```


``` 
vi /etc/sysconfig/network-scripts/ifcfg-eth{X} [X 为网卡序号]
TYPE=Ethernet
NAME=ethX
DEVICE=ethX
ONBOOT=yes
BOOTPROTO=none
MASTER=team0
SLAVE=yes
```

loadbalance

```
DEVICE=team1
DEVICETYPE=Team
ONBOOT=yes
BOOTPROTO=none
IPADDR=<外部网络 IP>
NETMASK=<子网掩码>
GATEWAY=<网关地址>
DNS1=<首选 DNS 服务器>
DNS2=<备选 DNS 服务器>
TEAM_CONFIG='{"runner": {"name": "loadbalance"}}'
```

```
vi /etc/sysconfig/network-scripts/ifcfg-eth{X} [X 为网卡序号]
TYPE=Ethernet
NAME=ethX
DEVICE=ethX
ONBOOT=yes
BOOTPROTO=none
MASTER=team1
SLAVE=yes
```

重启网络

```
sudo systemctl restart NetworkManager
```


## Rocky 9 & Centos 9

- activebackup

```
# 创建 Team 接口
sudo nmcli con add type team con-name team0 ifname team0 config '{"runner": {"name": "activebackup"}}'

# 配置 IP 地址、子网掩码、网关和 DNS
sudo nmcli con mod team0 ipv4.method manual ipv4.address <管理内网 IP>/<子网掩码> ipv4.gateway <网关地址> ipv4.dns "<首选 DNS 服务器>,<备选 DNS 服务器>"

# 添加队员接口
sudo nmcli con add type ethernet con-name eth0 ifname eth0 master team0
sudo nmcli con add type ethernet con-name eth1 ifname eth1 master team0

# 启用 Team 接口
sudo nmcli con up team0
```

- loadbalance

```
# 创建 Team 接口
sudo nmcli con add type team con-name team1 ifname team1 config '{"runner": {"name": "loadbalance"}}'

# 配置 IP 地址、子网掩码、网关和 DNS
sudo nmcli con mod team1 ipv4.method manual ipv4.address <外部网络 IP>/<子网掩码> ipv4.gateway <网关地址> ipv4.dns "<首选 DNS 服务器>,<备选 DNS 服务器>"

# 添加队员接口
sudo nmcli con add type ethernet con-name eth2 ifname eth0 master team1
sudo nmcli con add type ethernet con-name eth3 ifname eth1 master team1

# 启用 Team 接口
sudo nmcli con up team1
```

## 参数配置

### 1. 设置 all-available-devices

```
ceph orch apply osd --all-available-devices --unmanaged=true
```

## Ceph Crush 规则基础玩法

### 1. Ceph 使用规则创建池
在Ceph中，你可以将SSD设置为日志盘（用于存储日志和元数据）和HDD设置为数据盘（用于存储实际数据）。这通常涉及到使用Ceph的CRUSH映射算法和OSD层面的设置。以下是一般的步骤：

1. **设置CRUSH Map**:
    修改CRUSH映射，确保SSD和HDD有不同的存储池，并且CRUSH规则将SSD和HDD映射到不同的设备类别（例如，`ssd-device-class` 和 `hdd-device-class`）。
  
      ```bash
      # 查看设备类别
      ceph osd crush class ls # 删除 ceph osd crush class rm hdd
      [
        "ssd"
      ]

      # 查看 osd 所属设备类别
      ceph osd crush class ls-osd ssd
      0
      1
      2
      3
      4
      5
      6
      7
      8
      ```

      ```bash
      # 创建设备类别
      ceph osd crush class create ssd-device-class
      ceph osd crush class create hdd-device-class

      # 查看 OSD 对应的设备类别
      # 观察到这里所有的设备类别均为 ssd
      ceph osd tree
      ID   CLASS  WEIGHT    TYPE NAME       STATUS  REWEIGHT  PRI-AFF
      -1         62.87668  root default
      -7         13.97260      host ceph1
        2    ssd   6.98630          osd.2       up   1.00000  1.00000
        3    ssd   6.98630          osd.3       up   1.00000  1.00000
      -3                0      host ceph2
      -5                0      host ceph3
      -13         13.97260      host ceph4
        0    ssd   6.98630          osd.0       up   1.00000  1.00000
        1    ssd   6.98630          osd.1       up   1.00000  1.00000
      -9         13.97260      host ceph5
        4    ssd   6.98630          osd.4       up   1.00000  1.00000
        5    ssd   6.98630          osd.5       up   1.00000  1.00000
      -11         20.95889      host ceph6
        6    ssd   6.98630          osd.6       up   1.00000  1.00000
        7    ssd   6.98630          osd.7       up   1.00000  1.00000
        8    ssd   6.98630          osd.8       up   1.00000  1.00000
      

      # 删除设备对应的设备类别
      ceph osd crush rm-device-class osd.1

      # 添加设备类别<笔记部分，参考作用>
      # 1. 创建设备类别
      # 2. 添加设备类别（需要设备的设备类别为空）
      ceph osd crush class create ssd-device-class
      ceph osd crush set-device-class ssd-device-class

      # 创建一个优先使用 SSD 设备的 cursh rule
      # 创建了一个rule的名字为：rule-ssd，在root名为default下的rule
      ceph osd crush rule create-replicated rule-ssd default  host ssd 
      ceph osd crush rule ls # 查看 rule 列表

      Outputs: >>
      replicated_rule
      rule-ssd

      # 下载集群crushmap 查看发生的变化
      # 下面的工具需要安装全量的 ceph 工具包 yum install ceph
      ceph osd getcrushmap -o crushmap
      crushtool -d crushmap -o crushmap
      cat crushmap

      # 创建一个使用该rule-ssd规则的存储池：
      ceph osd pool create ssdpool 64 64 rule-ssd

      # 查看ssdpool的信息可以看到使用的crush_rule 为1，也就是 rule-ssd
      ceph osd pool ls detail


      # 创建一个对象test并放到ssdpool中：
      rados -p ssdpool ls
      echo "hahah" >test.txt
      rados -p ssdpool put test test.txt 
      rados -p ssdpool ls

      # 查看该对象的osd组：
      ceph osd map ssdpool test
      ```

      :::details ceph crush rule

      ```bash
      # 查看 Ceph OSD CRUSH RULE
      ceph osd crush rule ls
      replicated_rule
      ```


      :::

2. **创建存储池**:
    使用`rados`命令或Ceph的管理工具创建SSD和HDD存储池。

    ```bash
    # 创建SSD存储池
    ceph osd pool create ssd-pool 128 128 replicated ssd-device-class-replicated

    # 创建HDD存储池
    ceph osd pool create hdd-pool 128 128 replicated hdd-device-class-replicated
    ```

3. **将OSD节点与设备类别关联**:
    确保SSD和HDD的OSD节点与相应的设备类别关联起来。这通常需要在Ceph的配置文件中进行设置。

4. **创建RBD卷**:
    使用`rbd`命令创建RBD（RADOS Block Device）卷。例如：

    ```bash
    # 在SSD池中创建RBD卷
    rbd create my-ssd-volume --size 1024 --pool ssd-pool --image-feature layering

    # 在HDD池中创建RBD卷
    rbd create my-hdd-volume --size 2048 --pool hdd-pool --image-feature layering
    ```

:::tip 请注意
具体的命令和配置可能会根据你的Ceph版本和集群配置略有不同。确保查阅Ceph的官方文档或相应版本的文档以获取详细的指导。
:::

### 2. 高速缓冲池与数据池结合（SSD+HDD）

1. 配置以 cache 作为 sata-pool 的前端高速缓冲池
   1. 新建缓冲池，其中，cache 作为 sata-pool 的前端高速缓冲池。
        ```bash
        ceph osd pool create storage 64
        ceph osd pool create cache 64
        ```
    2. 设定缓冲池读写策略为写回模式
        ```bash
        ceph osd tier cache-mode cache writeback
        ```
    3. 把缓存层挂接到后端存储池上
        ```bash
        ceph osd tier add storage cache
        ```
    4. 将客户端流量指向到缓存存储池
        ```bash
        ceph osd tier set-overlay storage cache
        ```
   
2. 调整 Cache tier 配置
   1. 设置缓存层 hit_set_type 使用 bloom 过滤器
      ```bash
      ceph osd pool set cache hit_set_type bloom
      # 命令格式如下：
      ceph osd pool set {cachepool} {key} {value}
      # 关于 Bloom-Filte 算法原理可参见：
      https://blog.csdn.net/jiaomeng/article/details/1495500
      ```
   2. 设置 hit_set_count、hit_set_period、target_max_bytes
      ```bash
      ceph osd pool set cache hit_set_count 1
      # set pool 27 hit_set_count to 1
      ceph osd pool set cache hit_set_period 3600
      # set pool 27 hit_set_period to 3600
      ceph osd pool set cache target_max_bytes 1000000000000
      # set pool 27 target_max_bytes to 1000000000000
      ```
      :::tip
      默认情况下缓冲池基于数据的修改时间来进行确定是否命中缓存，也可以设定热度数 hit_set_count 和热度周期 hit_set_period，以及最大缓冲数据 target_max_bytes。hit_set_count 和 hit_set_period 选项分别定义了 HitSet 覆盖的时间区间、以及保留多少个这样的 HitSet，保留一段时间以来的访问记录，这样 Ceph 就能判断一客户端在一段时间内访问了某对象一次、还是多次（存活期与热度）。
      :::
   3. 设置 min_read_recency_for_promete、min_write_recency_for_promote
      ```bash
      ceph osd pool set cache min_read_recency_for_promote 1
      # set pool 27 min_read_recency_for_promote to 1
      ceph osd pool set cache min_write_recency_for_promote 1
      # set pool 27 min_write_recency_for_promote to 1
      ```
      :::tip
      缓存池容量控制

      先讲解个概念缓存池代理层两大主要操作

      - 刷写（flushing）：负责把已经被修改的对象写入到后端慢存储，但是对象依然在缓冲池。
      - 驱逐（evicting）：负责在缓冲池里销毁那些没有被修改的对象。


      --- 
      1. 缓冲池代理层进行刷写和驱逐的操作，主要和缓冲池本身的容量有关。在缓冲池里，如果被修改的数据达到一个阈值（阈值（容量百分比），缓冲池代理就开始把这些数据刷写到后端慢存储。当缓冲池里被修改的数据达到 40% 时，则触发刷写动作。
      ```bash
      ceph osd pool set cache cache_target_dirty_ratio 0.4
      ```

      2. 当被修改的数据达到一个确定的阈值（容量百分比），刷写动作将会以高速运作。例如，当缓冲池里被修改数据达到 60% 时候，则高速刷写。
      ```
      ceph osd pool set cache cache_target_dirty_high_ratio 0.6
      ```

      3. 缓冲池的代理将会触发驱逐操作，目的是释放缓冲区空间。例如，当缓冲池里的容量使用达到 80% 时候，则触发驱逐操作。
      ```bash
      ceph osd pool set cache cache_target_full_ratio 0.8
      ```

      4. 除了上面提及基于缓冲池的百分比来判断是否触发刷写和驱逐，还可以指定确定的数据对象数量或者确定的数据容量。对缓冲池设定最大的数据容量，来强制触发刷写和驱逐操作。
      ```bash
      ceph osd pool set cache target_max_bytes 1073741824   
      ```

      5. 同时，也可以对缓冲池设定最大的对象数量。在默认情况下，RBD 的默认对象大小为 4MB，1GB 容量包含 256 个 4MB 的对象，则可以设定：
      ```
      ceph osd pool set cache target_max_objects 256
      ```
      :::

   4. 缓冲池的数据刷新问题在缓冲池里，对象有最短的刷写周期。若被修改的对象在缓冲池里超过最短周期，将会被刷写到慢存储池。
      ```bash
      ceph osd pool set cache cache_min_flush_ age 600 # 单位（分钟）
      ```
      
      设定对象最短的驱逐周期
      ```bash
      ceph osd pool set cache cache_min_evict_age 1800
      ```

3. 删除缓存层
   1. 删除 readonly 缓存
      1. 把缓存模式改为 none 即可禁用。
         ```bash
         ceph osd tier cache-mode {cachepool} none
         ```
     
      2. 去除后端存储池的缓存池。
         ```bash
         ceph osd tier remove {storagepool} {cachepool}
         ```

   2. 删除 writeback 缓存
      1. 把缓存模式改为 forward ，这样新的和更改过的对象将直接刷回到后端存储池
          ```bash
          ceph osd tier cache-mode cache forward --yes-i-really-mean-it
          # set cache-mode for pool 'cache' to forward
          ```
      2. 确保缓存池已刷回，可能要等数分钟
          ```bash
          rados ls -p cache

          # 可以通过以下命令进行手动刷回
          rados -p cache cache-flush-evict-all
          ```
      3. 取消流量指向缓存池
          ```bash
          ceph osd tier remove-overlay storage
          # there is now (or already was) no overlay for 'storage'
          ```
      4. 剥离缓存池
          ```bash
          ceph osd tier remove storage cache
          # pool 'cache' is now (or already was) not a tier of 'storage'
          ```

## 引用

- [ceph 优化配置](https://blog.csdn.net/wjandy0211/article/details/85013497#:~:text=read_ahead%2C%20%E9%80%9A%E8%BF%87%E6%95%B0%E6%8D%AE%E9%A2%84%E8%AF%BB%E5%B9%B6%E4%B8%94%E8%AE%B0%E8%BD%BD%E5%88%B0%E9%9A%8F%E6%9C%BA%E8%AE%BF%E9%97%AE%E5%86%85%E5%AD%98%E6%96%B9%E5%BC%8F%E6%8F%90%E9%AB%98%E7%A3%81%E7%9B%98%E8%AF%BB%E6%93%8D%E4%BD%9C%EF%BC%8C%E6%9F%A5%E7%9C%8B%E9%BB%98%E8%AE%A4%E5%80%BC%20cat%20%2F%20sys%20%2Fblock%2F%20sda%20%2F,%2F%20sys%20%2Fblock%2F%20sda%20%2F%20queue%20%2Fread%20_ahead_kb)

- [Ceph 场景选择](https://blog.csdn.net/wylfengyujiancheng/article/details/88299773)

- [存储分层在 OpenStack 的一个应用](https://www.xiaocoder.com/2017/09/05/ceph-for-openstack-storage/)
  
- [ceph 的通用解决方案系列-3](https://blog.csdn.net/wylfengyujiancheng/article/details/88305702)