---
title: Ceph 集群优化
date: 2023-08-16 11:32:50
permalink: /pages/bc143f/
categories:
  - Ceph
tags:
  - 
author: 
  name: brook-w
  link: https://github.com/brook-w
---

## 1. 内核优化

### 1. [块设备优化](https://www.xmmup.com/linuxyouhuazhicipanyudureadahead.html)

```sh
blockdev -V
blockdev --report [设备]
blockdev [-v|-q] 命令 设备

--getsz                   获得 512-字节扇区的大小 
--setro                   设置只读 
--setrw                   设置读写 
--getro                   获得只读 
--getdiscardzeroes        获取 忽略零数据 支持状态 
--getss                   获得逻辑块(扇区)大小 
--getpbsz                 获得物理块(扇区)大小 
--getiomin                获得最小 I/O 大小 
--getioopt                获得最优 I/O 大小 
--getalignoff             获得对齐偏移字节数 
--getmaxsect              获得每次请求的最大扇区数 
--getbsz                  获得块大小 
--setbsz                  set blocksize on file descriptor opening the block device --getsize                 获得 32-bit 扇区数量(已废弃，请使用 --getsz) 
--getsize64               获得字节大小 
--setra                   设置 readahead 
--getra                   获取 readahead 
--setfra                  设置文件系统 readahead 
--getfra                  获取文件系统 readahead 
--flushbufs               刷新缓存 
--rereadpt                重新读取分区表
```

### 2. 块设备预读大小

```sh
/sbin/blockdev --getra /dev/sda
cat /sys/class/block/sda/queue/read_ahead_kb

blockdev --report
```




## 2. 网络配置

Ceph 集群中有两个网络（管理内网和外部网络）每个节点都有 4 个 10G 光口。

管理内网：
由于管理网络主要用于集群节点之间的通信、数据同步和控制命令传输，稳定性和可靠性非常重要。在这个网络上，推荐使用 "activebackup" 运行者模式。

- 冗余性：该模式提供了故障切换和冗余性，以确保在一个主接口出现故障时能够切换到备用接口，保持管理网络的连通性。
- 稳定性：管理网络的稳定性是至关重要的，因为它直接影响到集群节点之间的通信和控制。

外部网络：

外部网络通常用于与其他集群或外部系统进行通信，以及为 OpenStack 集群等提供网络连接。在这个网络上，你可以考虑使用 "loadbalance" 运行者模式。

- 带宽利用：外部网络通常需要支持大量的数据传输，负载均衡模式可以更好地利用多个 10G 光口的带宽，从而提高整体的网络性能。
- 高吞吐量："loadbalance" 模式适用于需要处理大量数据的场景，这对于外部网络非常适用。

activebackup

```
vi /etc/sysconfig/network-scripts/ifcfg-team0
DEVICE=team0
DEVICETYPE=Team
ONBOOT=yes
BOOTPROTO=none
IPADDR=<管理内网 IP>
NETMASK=<子网掩码>
GATEWAY=<网关地址>
DNS1=<首选 DNS 服务器>
DNS2=<备选 DNS 服务器>
TEAM_CONFIG='{"runner": {"name": "activebackup"}}'
```


``` 
vi /etc/sysconfig/network-scripts/ifcfg-eth{X} [X 为网卡序号]
TYPE=Ethernet
NAME=ethX
DEVICE=ethX
ONBOOT=yes
BOOTPROTO=none
MASTER=team0
SLAVE=yes
```

loadbalance

```
DEVICE=team1
DEVICETYPE=Team
ONBOOT=yes
BOOTPROTO=none
IPADDR=<外部网络 IP>
NETMASK=<子网掩码>
GATEWAY=<网关地址>
DNS1=<首选 DNS 服务器>
DNS2=<备选 DNS 服务器>
TEAM_CONFIG='{"runner": {"name": "loadbalance"}}'
```

```
vi /etc/sysconfig/network-scripts/ifcfg-eth{X} [X 为网卡序号]
TYPE=Ethernet
NAME=ethX
DEVICE=ethX
ONBOOT=yes
BOOTPROTO=none
MASTER=team1
SLAVE=yes
```

重启网络

```
sudo systemctl restart NetworkManager
```


## Rocky 9 & Centos 9

- activebackup

```
# 创建 Team 接口
sudo nmcli con add type team con-name team0 ifname team0 config '{"runner": {"name": "activebackup"}}'

# 配置 IP 地址、子网掩码、网关和 DNS
sudo nmcli con mod team0 ipv4.method manual ipv4.address <管理内网 IP>/<子网掩码> ipv4.gateway <网关地址> ipv4.dns "<首选 DNS 服务器>,<备选 DNS 服务器>"

# 添加队员接口
sudo nmcli con add type ethernet con-name eth0 ifname eth0 master team0
sudo nmcli con add type ethernet con-name eth1 ifname eth1 master team0

# 启用 Team 接口
sudo nmcli con up team0
```

- loadbalance

```
# 创建 Team 接口
sudo nmcli con add type team con-name team1 ifname team1 config '{"runner": {"name": "loadbalance"}}'

# 配置 IP 地址、子网掩码、网关和 DNS
sudo nmcli con mod team1 ipv4.method manual ipv4.address <外部网络 IP>/<子网掩码> ipv4.gateway <网关地址> ipv4.dns "<首选 DNS 服务器>,<备选 DNS 服务器>"

# 添加队员接口
sudo nmcli con add type ethernet con-name eth2 ifname eth0 master team1
sudo nmcli con add type ethernet con-name eth3 ifname eth1 master team1

# 启用 Team 接口
sudo nmcli con up team1
```

## 参数配置

### 1. 设置 all-available-devices

```
ceph orch apply osd --all-available-devices --unmanaged=true
```

## Ceph Crush 规则基础玩法

### 1. Ceph 使用规则创建池
在Ceph中，你可以将SSD设置为日志盘（用于存储日志和元数据）和HDD设置为数据盘（用于存储实际数据）。这通常涉及到使用Ceph的CRUSH映射算法和OSD层面的设置。以下是一般的步骤：

1. **设置CRUSH Map**:
    修改CRUSH映射，确保SSD和HDD有不同的存储池，并且CRUSH规则将SSD和HDD映射到不同的设备类别（例如，`ssd-device-class` 和 `hdd-device-class`）。
  
      ```bash
      # 查看设备类别
      ceph osd crush class ls # 删除 ceph osd crush class rm hdd
      [
        "ssd"
      ]

      # 查看 osd 所属设备类别
      ceph osd crush class ls-osd ssd
      0
      1
      2
      3
      4
      5
      6
      7
      8
      ```

      ```bash
      # 创建设备类别
      ceph osd crush class create ssd-device-class
      ceph osd crush class create hdd-device-class

      # 查看 OSD 对应的设备类别
      # 观察到这里所有的设备类别均为 ssd
      ceph osd tree
      ID   CLASS  WEIGHT    TYPE NAME       STATUS  REWEIGHT  PRI-AFF
      -1         62.87668  root default
      -7         13.97260      host ceph1
        2    ssd   6.98630          osd.2       up   1.00000  1.00000
        3    ssd   6.98630          osd.3       up   1.00000  1.00000
      -3                0      host ceph2
      -5                0      host ceph3
      -13         13.97260      host ceph4
        0    ssd   6.98630          osd.0       up   1.00000  1.00000
        1    ssd   6.98630          osd.1       up   1.00000  1.00000
      -9         13.97260      host ceph5
        4    ssd   6.98630          osd.4       up   1.00000  1.00000
        5    ssd   6.98630          osd.5       up   1.00000  1.00000
      -11         20.95889      host ceph6
        6    ssd   6.98630          osd.6       up   1.00000  1.00000
        7    ssd   6.98630          osd.7       up   1.00000  1.00000
        8    ssd   6.98630          osd.8       up   1.00000  1.00000
      

      # 删除设备对应的设备类别
      ceph osd crush rm-device-class osd.1

      # 添加设备类别<笔记部分，参考作用>
      # 1. 创建设备类别
      # 2. 添加设备类别（需要设备的设备类别为空）
      ceph osd crush class create ssd-device-class
      ceph osd crush set-device-class ssd-device-class

      # 创建一个优先使用 SSD 设备的 cursh rule
      # 创建了一个rule的名字为：rule-ssd，在root名为default下的rule
      ceph osd crush rule create-replicated rule-ssd default  host ssd 
      ceph osd crush rule ls # 查看 rule 列表

      Outputs: >>
      replicated_rule
      rule-ssd

      # 下载集群crushmap 查看发生的变化
      # 下面的工具需要安装全量的 ceph 工具包 yum install ceph
      ceph osd getcrushmap -o crushmap
      crushtool -d crushmap -o crushmap
      cat crushmap

      # 创建一个使用该rule-ssd规则的存储池：
      ceph osd pool create ssdpool 64 64 rule-ssd

      # 查看ssdpool的信息可以看到使用的crush_rule 为1，也就是 rule-ssd
      ceph osd pool ls detail


      # 创建一个对象test并放到ssdpool中：
      rados -p ssdpool ls
      echo "hahah" >test.txt
      rados -p ssdpool put test test.txt 
      rados -p ssdpool ls

      # 查看该对象的osd组：
      ceph osd map ssdpool test
      ```

      :::details ceph crush rule

      ```bash
      # 查看 Ceph OSD CRUSH RULE
      ceph osd crush rule ls
      replicated_rule
      ```


      :::

2. **创建存储池**:
    使用`rados`命令或Ceph的管理工具创建SSD和HDD存储池。

    ```bash
    # 创建SSD存储池
    ceph osd pool create ssd-pool 128 128 replicated ssd-device-class-replicated

    # 创建HDD存储池
    ceph osd pool create hdd-pool 128 128 replicated hdd-device-class-replicated
    ```

3. **将OSD节点与设备类别关联**:
    确保SSD和HDD的OSD节点与相应的设备类别关联起来。这通常需要在Ceph的配置文件中进行设置。

4. **创建RBD卷**:
    使用`rbd`命令创建RBD（RADOS Block Device）卷。例如：

    ```bash
    # 在SSD池中创建RBD卷
    rbd create my-ssd-volume --size 1024 --pool ssd-pool --image-feature layering

    # 在HDD池中创建RBD卷
    rbd create my-hdd-volume --size 2048 --pool hdd-pool --image-feature layering
    ```

:::tip 请注意
具体的命令和配置可能会根据你的Ceph版本和集群配置略有不同。确保查阅Ceph的官方文档或相应版本的文档以获取详细的指导。
:::

### 2. 高速缓冲池与数据池结合（SSD+HDD）

## 引用

- [ceph 优化配置](https://blog.csdn.net/wjandy0211/article/details/85013497#:~:text=read_ahead%2C%20%E9%80%9A%E8%BF%87%E6%95%B0%E6%8D%AE%E9%A2%84%E8%AF%BB%E5%B9%B6%E4%B8%94%E8%AE%B0%E8%BD%BD%E5%88%B0%E9%9A%8F%E6%9C%BA%E8%AE%BF%E9%97%AE%E5%86%85%E5%AD%98%E6%96%B9%E5%BC%8F%E6%8F%90%E9%AB%98%E7%A3%81%E7%9B%98%E8%AF%BB%E6%93%8D%E4%BD%9C%EF%BC%8C%E6%9F%A5%E7%9C%8B%E9%BB%98%E8%AE%A4%E5%80%BC%20cat%20%2F%20sys%20%2Fblock%2F%20sda%20%2F,%2F%20sys%20%2Fblock%2F%20sda%20%2F%20queue%20%2Fread%20_ahead_kb)

- [Ceph 场景选择](https://blog.csdn.net/wylfengyujiancheng/article/details/88299773)

- [存储分层在 OpenStack 的一个应用](https://www.xiaocoder.com/2017/09/05/ceph-for-openstack-storage/)
  
- [ceph 的通用解决方案系列-3](https://blog.csdn.net/wylfengyujiancheng/article/details/88305702)